{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WBjMXf9HHnB6"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import matplotlib.patches as patches\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import ops\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To use in Colab\n",
        "!git clone https://github.com/gabriellecaillaud/APS360_Traffic_Sign_Recognition.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXJwHsAJth9j",
        "outputId": "45323caa-51c0-4281-de9e-fbc4204a0cab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'APS360_Traffic_Sign_Recognition' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the custom modules on colab based on the files on git\n",
        "import sys\n",
        " #To use on colab\n",
        "sys.path.append('/content/APS360_Traffic_Sign_Recognition')\n",
        "\n",
        "# might be neccessary to change to from RCNN_model.utils import * in model\n",
        "from RCNN_model.model import *\n",
        "from RCNN_model.utils import *"
      ],
      "metadata": {
        "id": "anSb-RmNIsXr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path csv with labels\n",
        "csv_path = \"/content/APS360_Traffic_Sign_Recognition/dataset_traffic_signs.csv\""
      ],
      "metadata": {
        "id": "pejgKAV2-OEt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load the csv file\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_val_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the train_val_data into train and validation sets\n",
        "train_data, val_data = train_test_split(train_val_data, test_size=0.25, random_state=42)\n",
        "\n",
        "# Save the train, validation, and test sets to csv files\n",
        "\n",
        "train_data.to_csv('train.csv', index=False)\n",
        "val_data.to_csv('val.csv', index=False)\n",
        "test_data.to_csv('test.csv', index=False)\n"
      ],
      "metadata": {
        "id": "nwrwEKOz-APP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The goal of this cell is to split the train.csv into five .csv files\n",
        "# to avoid the memory to be full\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the input CSV file into a pandas DataFrame\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "# Calculate the number of rows per output file\n",
        "rows_per_file = len(df) // 5\n",
        "\n",
        "# Split the DataFrame into five smaller DataFrames\n",
        "dfs = [df[i:i+rows_per_file] for i in range(0, len(df), rows_per_file)]\n",
        "\n",
        "# Write each smaller DataFrame to a CSV file\n",
        "for i, df in enumerate(dfs):\n",
        "    df.to_csv(f'train_part_{i+1}.csv', index=False)\n"
      ],
      "metadata": {
        "id": "PWKYrMbzJGVh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('train_part_1.csv')"
      ],
      "metadata": {
        "id": "jWhX6u7qJfql"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "Fm-roEkTJlTW",
        "outputId": "bb14f5bf-41ff-47e0-f9cd-0ac7b2123e47"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Id                     imageUrl  annotation.0.centerX  \\\n",
              "0    4063    AugDataNoRight/img350.png              0.312339   \n",
              "1    2336  AugData/NoEntry/img4377.png              0.215084   \n",
              "2     271   AugData/100kmh/img3084.png                   NaN   \n",
              "3    3161      AugData/Stop/img427.png              0.692130   \n",
              "4    1026    AugData/yield/img1532.png              0.574550   \n",
              "..    ...                          ...                   ...   \n",
              "547    98   AugData/100kmh/img1569.png                   NaN   \n",
              "548   503    AugData/100kmh/img545.png                   NaN   \n",
              "549  4180   AugDataNoRight/img4470.png              0.307841   \n",
              "550   116   AugData/100kmh/img1587.png                   NaN   \n",
              "551  4060    AugDataNoRight/img347.png              0.641388   \n",
              "\n",
              "     annotation.0.centerY  annotation.0.width  annotation.0.height  \\\n",
              "0                0.444087            0.089974             0.106684   \n",
              "1                0.541589            0.098696             0.124767   \n",
              "2                     NaN                 NaN                  NaN   \n",
              "3                0.351235            0.186728             0.194444   \n",
              "4                0.333548            0.185090             0.176093   \n",
              "..                    ...                 ...                  ...   \n",
              "547                   NaN                 NaN                  NaN   \n",
              "548                   NaN                 NaN                  NaN   \n",
              "549              0.142674            0.170951             0.192802   \n",
              "550                   NaN                 NaN                  NaN   \n",
              "551              0.608612            0.107969             0.122108   \n",
              "\n",
              "    annotation.0.classification  annotation.1.centerX  annotation.1.centerY  \\\n",
              "0                       noRight                   NaN                   NaN   \n",
              "1                       NoEntry                   NaN                   NaN   \n",
              "2                           NaN                   NaN                   NaN   \n",
              "3                          Stop                   NaN                   NaN   \n",
              "4                         yield                   NaN                   NaN   \n",
              "..                          ...                   ...                   ...   \n",
              "547                         NaN                   NaN                   NaN   \n",
              "548                         NaN                   NaN                   NaN   \n",
              "549                     noRight                   NaN                   NaN   \n",
              "550                         NaN                   NaN                   NaN   \n",
              "551                     noRight                   NaN                   NaN   \n",
              "\n",
              "     annotation.1.width  annotation.1.height annotation.1.classification  \n",
              "0                   NaN                  NaN                         NaN  \n",
              "1                   NaN                  NaN                         NaN  \n",
              "2                   NaN                  NaN                         NaN  \n",
              "3                   NaN                  NaN                         NaN  \n",
              "4                   NaN                  NaN                         NaN  \n",
              "..                  ...                  ...                         ...  \n",
              "547                 NaN                  NaN                         NaN  \n",
              "548                 NaN                  NaN                         NaN  \n",
              "549                 NaN                  NaN                         NaN  \n",
              "550                 NaN                  NaN                         NaN  \n",
              "551                 NaN                  NaN                         NaN  \n",
              "\n",
              "[552 rows x 12 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d2d32fe1-6e39-465e-aaa3-a2eae55f3061\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>imageUrl</th>\n",
              "      <th>annotation.0.centerX</th>\n",
              "      <th>annotation.0.centerY</th>\n",
              "      <th>annotation.0.width</th>\n",
              "      <th>annotation.0.height</th>\n",
              "      <th>annotation.0.classification</th>\n",
              "      <th>annotation.1.centerX</th>\n",
              "      <th>annotation.1.centerY</th>\n",
              "      <th>annotation.1.width</th>\n",
              "      <th>annotation.1.height</th>\n",
              "      <th>annotation.1.classification</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4063</td>\n",
              "      <td>AugDataNoRight/img350.png</td>\n",
              "      <td>0.312339</td>\n",
              "      <td>0.444087</td>\n",
              "      <td>0.089974</td>\n",
              "      <td>0.106684</td>\n",
              "      <td>noRight</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2336</td>\n",
              "      <td>AugData/NoEntry/img4377.png</td>\n",
              "      <td>0.215084</td>\n",
              "      <td>0.541589</td>\n",
              "      <td>0.098696</td>\n",
              "      <td>0.124767</td>\n",
              "      <td>NoEntry</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>271</td>\n",
              "      <td>AugData/100kmh/img3084.png</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3161</td>\n",
              "      <td>AugData/Stop/img427.png</td>\n",
              "      <td>0.692130</td>\n",
              "      <td>0.351235</td>\n",
              "      <td>0.186728</td>\n",
              "      <td>0.194444</td>\n",
              "      <td>Stop</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1026</td>\n",
              "      <td>AugData/yield/img1532.png</td>\n",
              "      <td>0.574550</td>\n",
              "      <td>0.333548</td>\n",
              "      <td>0.185090</td>\n",
              "      <td>0.176093</td>\n",
              "      <td>yield</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>547</th>\n",
              "      <td>98</td>\n",
              "      <td>AugData/100kmh/img1569.png</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>548</th>\n",
              "      <td>503</td>\n",
              "      <td>AugData/100kmh/img545.png</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>549</th>\n",
              "      <td>4180</td>\n",
              "      <td>AugDataNoRight/img4470.png</td>\n",
              "      <td>0.307841</td>\n",
              "      <td>0.142674</td>\n",
              "      <td>0.170951</td>\n",
              "      <td>0.192802</td>\n",
              "      <td>noRight</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550</th>\n",
              "      <td>116</td>\n",
              "      <td>AugData/100kmh/img1587.png</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>551</th>\n",
              "      <td>4060</td>\n",
              "      <td>AugDataNoRight/img347.png</td>\n",
              "      <td>0.641388</td>\n",
              "      <td>0.608612</td>\n",
              "      <td>0.107969</td>\n",
              "      <td>0.122108</td>\n",
              "      <td>noRight</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>552 rows × 12 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d2d32fe1-6e39-465e-aaa3-a2eae55f3061')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d2d32fe1-6e39-465e-aaa3-a2eae55f3061 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d2d32fe1-6e39-465e-aaa3-a2eae55f3061');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "from tqdm import tqdm\n",
        "import pickle"
      ],
      "metadata": {
        "id": "7IeMW8Wg6KAp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ObjectDetectionDataset(Dataset):\n",
        "    '''\n",
        "    A Pytorch Dataset class to load the images and their corresponding annotations.\n",
        "    \n",
        "    Returns\n",
        "    ------------\n",
        "    images: torch.Tensor of size (B, C, H, W)\n",
        "    gt bboxes: torch.Tensor of size (B, max_objects, 4)\n",
        "    gt classes: torch.Tensor of size (B, max_objects)\n",
        "    '''\n",
        "    def __init__(self, csv_path, img_size, name2idx):\n",
        "        self.annotation_path = csv_path\n",
        "        self.img_size = img_size\n",
        "        self.name2idx = name2idx\n",
        "        \n",
        "        self.img_data_all, self.gt_bboxes_all, self.gt_classes_all = self.get_data()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.img_data_all.size(dim=0)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.img_data_all[idx], self.gt_bboxes_all[idx], self.gt_classes_all[idx]\n",
        "        \n",
        "    def get_data(self):\n",
        "        img_data_all = []\n",
        "        gt_idxs_all = []\n",
        "        \n",
        "        gt_boxes_all, gt_classes_all, img_paths = parse_annotation(self.annotation_path, self.img_size)\n",
        "        \n",
        "        for i, img_path in tqdm(enumerate(img_paths), total=len(img_paths)):\n",
        "      \n",
        "            # skip if the image path is not valid\n",
        "            if (not img_path) or (not os.path.exists(img_path)):\n",
        "                continue\n",
        "            \n",
        "            # read and resize image\n",
        "            \n",
        "            img = io.imread(img_path)\n",
        "            img = resize(img, self.img_size)\n",
        "            \n",
        "            # convert image to torch tensor and reshape it so channels come first\n",
        "            img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
        "            \n",
        "            # encode class names as integers\n",
        "            gt_classes = gt_classes_all[i]\n",
        "            gt_idx = torch.Tensor([self.name2idx[name] for name in gt_classes])\n",
        "            \n",
        "            img_data_all.append(img_tensor)\n",
        "            gt_idxs_all.append(gt_idx)\n",
        "        \n",
        "        # pad bounding boxes and classes so they are of the same size\n",
        "\n",
        "        if len(gt_boxes_all)!=0 and len(gt_idxs_all)!=0 :\n",
        "          \n",
        "          gt_bboxes_pad = pad_sequence(gt_boxes_all, batch_first=True, padding_value=-1)\n",
        "          gt_classes_pad = pad_sequence(gt_idxs_all, batch_first=True, padding_value=-1)\n",
        "        \n",
        "        # stack all images\n",
        "        img_data_stacked = torch.stack(img_data_all)[:, :3, :, :]\n",
        "        \n",
        "        return img_data_stacked.to(dtype=torch.float32), gt_bboxes_pad, gt_classes_pad\n",
        "\n",
        "    def __getstate__(self):\n",
        "        state = self.__dict__.copy()\n",
        "        state['img_data_all'] = pickle.dumps(state['img_data_all'])\n",
        "        state['gt_bboxes_all'] = pickle.dumps(state['gt_bboxes_all'])\n",
        "        state['gt_classes_all'] = pickle.dumps(state['gt_classes_all'])\n",
        "        return state\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        state['img_data_all'] = pickle.loads(state['img_data_all'])\n",
        "        state['gt_bboxes_all'] = pickle.loads(state['gt_bboxes_all'])\n",
        "        state['gt_classes_all'] = pickle.loads(state['gt_classes_all'])\n",
        "        self.__dict__.update(state)"
      ],
      "metadata": {
        "id": "1O6UG0eCH1LN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_width = 640\n",
        "img_height = 480\n",
        "csv_path = \"/content/APS360_Traffic_Sign_Recognition/dataset_traffic_signs.csv\"\n",
        "image_dir = os.path.join(\"data\", \"images\")\n",
        "name2idx = {'pad': -1, '30kmh': 0,'60kmh':1, '100kmh' : 2, 'yield': 3, 'keepRight' :4, 'NoEntry':5, 'NoLeft': 6, 'Stop':7, 'noRight':8, 'ChildrenCrossing' :9 }\n",
        "idx2name = {v:k for k, v in name2idx.items()}"
      ],
      "metadata": {
        "id": "LobuJxwKH3d-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a Pytorch Dataset file using custom class\n",
        "\n",
        "od_dataset_train_1 = ObjectDetectionDataset(\"train_part_1.csv\", (img_height, img_width), name2idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJYGNJRLJzSd",
        "outputId": "b8726d1f-9c07-420d-da26-33a91d1c3898"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 463/463 [00:39<00:00, 11.61it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#trying to save the dataset. It won't work on Colab, the memory will be full\n",
        "torch.save(od_dataset_train_1, 'od_dataset_train_1.pt')"
      ],
      "metadata": {
        "id": "-KCtmaCAOSnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a second subdataset\n",
        "od_dataset_train_2 = ObjectDetectionDataset(\"train_part_2.csv\", (img_height, img_width), name2idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IlWedJ3KeIV",
        "outputId": "710bb756-73f3-4f26-ff2d-a68f0c983ca1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 436/436 [00:35<00:00, 12.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#concatenating the two  small datasets\n",
        "train_dev_sets = torch.utils.data.ConcatDataset([od_dataset_train_1, od_dataset_train_2])\n",
        "#train_dev_loader = DataLoader(dataset=train_dev_sets, ...)"
      ],
      "metadata": {
        "id": "POQrvRhFK64a"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#useful for the model definition\n",
        "out_c, out_h, out_w = 2048 ,15 ,20\n"
      ],
      "metadata": {
        "id": "b0zCK8Iby0Cu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataloading\n",
        "train_data1_loader = DataLoader(od_dataset_train_1, batch_size=16)"
      ],
      "metadata": {
        "id": "I7ZkNhRXOr7G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = (img_height, img_width)\n",
        "out_size = (out_h, out_w) ## see other d\n",
        "n_classes = len(name2idx) - 1 # exclude pad idx\n",
        "roi_size = (2, 2)\n",
        "\n",
        "#the model\n",
        "detector = TwoStageDetector(img_size, out_size, out_c, n_classes, roi_size)"
      ],
      "metadata": {
        "id": "LflolHzGyq2p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5064f744-24bd-4904-8d6a-6efef0810471"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 318MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "YwQEyZQjQE-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(model, learning_rate, train_dataloader, n_epochs):\n",
        "    \n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    \n",
        "    for i in tqdm(range(n_epochs)):\n",
        "        total_loss = 0\n",
        "        for img_batch, gt_bboxes_batch, gt_classes_batch in train_dataloader:\n",
        "            \n",
        "            # forward pass\n",
        "            loss = model(img_batch, gt_bboxes_batch, gt_classes_batch)\n",
        "            \n",
        "            # backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "        \n",
        "        loss_list.append(total_loss)\n",
        "        \n",
        "    return loss_list"
      ],
      "metadata": {
        "id": "ci8gtel1wfAI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-1\n",
        "n_epochs = 2\n",
        "\n",
        "loss_list = training_loop(detector, learning_rate, train_data1_loader, n_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd65tHTvPdKR",
        "outputId": "1fa94b7b-18ff-4fc1-9f5b-d573acfb5d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 1/2 [21:22<21:22, 1282.36s/it]"
          ]
        }
      ]
    }
  ]
}