{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WBjMXf9HHnB6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import matplotlib.patches as patches\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import ops\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To use in Colab\n",
        "!git clone https://github.com/gabriellecaillaud/APS360_Traffic_Sign_Recognition.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXJwHsAJth9j",
        "outputId": "786d3dd9-33b9-427e-c5e6-823851db2d6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'APS360_Traffic_Sign_Recognition' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        " #To use on colab\n",
        "sys.path.append('/content/APS360_Traffic_Sign_Recognition')\n",
        "from RCNN_model.model import *\n",
        "from RCNN_model.utils import *"
      ],
      "metadata": {
        "id": "anSb-RmNIsXr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "7IeMW8Wg6KAp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ObjectDetectionDataset(Dataset):\n",
        "    '''\n",
        "    A Pytorch Dataset class to load the images and their corresponding annotations.\n",
        "    \n",
        "    Returns\n",
        "    ------------\n",
        "    images: torch.Tensor of size (B, C, H, W)\n",
        "    gt bboxes: torch.Tensor of size (B, max_objects, 4)\n",
        "    gt classes: torch.Tensor of size (B, max_objects)\n",
        "    '''\n",
        "    def __init__(self, csv_path, img_size, name2idx):\n",
        "        self.annotation_path = csv_path\n",
        "        self.img_size = img_size\n",
        "        self.name2idx = name2idx\n",
        "        \n",
        "        self.img_data_all, self.gt_bboxes_all, self.gt_classes_all = self.get_data()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.img_data_all.size(dim=0)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.img_data_all[idx], self.gt_bboxes_all[idx], self.gt_classes_all[idx]\n",
        "        \n",
        "    def get_data(self):\n",
        "        img_data_all = []\n",
        "        gt_idxs_all = []\n",
        "        \n",
        "        gt_boxes_all, gt_classes_all, img_paths = parse_annotation(self.annotation_path, self.img_size)\n",
        "        \n",
        "        for i, img_path in tqdm(enumerate(img_paths), total=len(img_paths)):\n",
        "      \n",
        "            # skip if the image path is not valid\n",
        "            if (not img_path) or (not os.path.exists(img_path)):\n",
        "                continue\n",
        "            \n",
        "            # read and resize image\n",
        "            \n",
        "            img = io.imread(img_path)\n",
        "            img = resize(img, self.img_size)\n",
        "            \n",
        "            # convert image to torch tensor and reshape it so channels come first\n",
        "            img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
        "            \n",
        "            # encode class names as integers\n",
        "            gt_classes = gt_classes_all[i]\n",
        "            gt_idx = torch.Tensor([self.name2idx[name] for name in gt_classes])\n",
        "            \n",
        "            img_data_all.append(img_tensor)\n",
        "            gt_idxs_all.append(gt_idx)\n",
        "        \n",
        "        # pad bounding boxes and classes so they are of the same size\n",
        "\n",
        "        if len(gt_boxes_all)!=0 and len(gt_idxs_all)!=0 :\n",
        "          \n",
        "          gt_bboxes_pad = pad_sequence(gt_boxes_all, batch_first=True, padding_value=-1)\n",
        "          gt_classes_pad = pad_sequence(gt_idxs_all, batch_first=True, padding_value=-1)\n",
        "        \n",
        "        # stack all images\n",
        "        print(img_data_all)\n",
        "        img_data_stacked = torch.stack(img_data_all)[:, :3, :, :]\n",
        "        \n",
        "        return img_data_stacked.to(dtype=torch.float32), gt_bboxes_pad, gt_classes_pad"
      ],
      "metadata": {
        "id": "1O6UG0eCH1LN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_width = 640\n",
        "img_height = 480\n",
        "csv_path = \"/content/APS360_Traffic_Sign_Recognition/dataset_traffic_signs.csv\"\n",
        "image_dir = os.path.join(\"data\", \"images\")\n",
        "name2idx = {'pad': -1, '30kmh': 0,'60kmh':1, '100kmh' : 2, 'yield': 3, 'keepRight' :4, 'NoEntry':5, 'NoLeft': 6, 'Stop':7, 'noRight':8, 'ChildrenCrossing' :9 }\n",
        "idx2name = {v:k for k, v in name2idx.items()}"
      ],
      "metadata": {
        "id": "LobuJxwKH3d-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(csv_path)"
      ],
      "metadata": {
        "id": "qG0XAUVoH_d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(3687, inplace=True)"
      ],
      "metadata": {
        "id": "tDw515qZ1oyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"df_data.csv\")"
      ],
      "metadata": {
        "id": "ENKnRj0f2Gz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[3686,:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1K0k7lx1gYz",
        "outputId": "d2cce6a7-876e-45c6-93fa-60585ff6a8b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Id                                                  3686\n",
              "imageUrl                       AugData/30kmh/img4207.png\n",
              "annotation.0.centerX                            0.671296\n",
              "annotation.0.centerY                            0.647531\n",
              "annotation.0.width                              0.194444\n",
              "annotation.0.height                             0.299383\n",
              "annotation.0.classification                        30kmh\n",
              "annotation.1.centerX                                 NaN\n",
              "annotation.1.centerY                                 NaN\n",
              "annotation.1.width                                   NaN\n",
              "annotation.1.height                                  NaN\n",
              "annotation.1.classification                          NaN\n",
              "Name: 3686, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIWQ6ux8vWow",
        "outputId": "63069396-7f77-42b7-ccdc-7ee2dbc180ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Id', 'imageUrl', 'annotation.0.centerX', 'annotation.0.centerY',\n",
              "       'annotation.0.width', 'annotation.0.height',\n",
              "       'annotation.0.classification', 'annotation.1.centerX',\n",
              "       'annotation.1.centerY', 'annotation.1.width', 'annotation.1.height',\n",
              "       'annotation.1.classification'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvRQQneY2Uv3",
        "outputId": "c78f090e-5e8e-4b62-9b3c-dd844d9bcf84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4606"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "od_dataset = ObjectDetectionDataset(csv_path, (img_height, img_width), name2idx)"
      ],
      "metadata": {
        "id": "s3QEDPZWvYMW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09af631d-f01a-4e0f-e412-aa276c6144a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 1205/3688 [01:51<03:42, 11.18it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_c, out_h, out_w = 2048 ,15 ,20\n"
      ],
      "metadata": {
        "id": "b0zCK8Iby0Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model import TwoStageDetector"
      ],
      "metadata": {
        "id": "-vJoHoHsyhjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = (img_height, img_width)\n",
        "out_size = (out_h, out_w) ## see other d\n",
        "n_classes = len(name2idx) - 1 # exclude pad idx\n",
        "roi_size = (2, 2)\n",
        "\n",
        "detector = TwoStageDetector(img_size, out_size, out_c, n_classes, roi_size)"
      ],
      "metadata": {
        "id": "LflolHzGyq2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(model, learning_rate, train_dataloader, n_epochs):\n",
        "    \n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    \n",
        "    for i in tqdm(range(n_epochs)):\n",
        "        total_loss = 0\n",
        "        for img_batch, gt_bboxes_batch, gt_classes_batch in train_dataloader:\n",
        "            \n",
        "            # forward pass\n",
        "            loss = model(img_batch, gt_bboxes_batch, gt_classes_batch)\n",
        "            \n",
        "            # backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "        \n",
        "        loss_list.append(total_loss)\n",
        "        \n",
        "    return loss_list"
      ],
      "metadata": {
        "id": "ci8gtel1wfAI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}