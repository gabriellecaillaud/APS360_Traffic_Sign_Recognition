{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetectionDataset(Dataset):\n",
    "    '''\n",
    "    A Pytorch Dataset class to load the images and their corresponding annotations.\n",
    "    \n",
    "    Returns\n",
    "    ------------\n",
    "    images: torch.Tensor of size (B, C, H, W)\n",
    "    gt bboxes: torch.Tensor of size (B, max_objects, 4)\n",
    "    gt classes: torch.Tensor of size (B, max_objects)\n",
    "    '''\n",
    "    def __init__(self, csv_path, img_size, name2idx, img_dir = None):\n",
    "        self.annotation_path = csv_path\n",
    "        self.img_size = img_size\n",
    "        self.name2idx = name2idx\n",
    "        \n",
    "        self.img_data_all, self.gt_bboxes_all, self.gt_classes_all = self.get_data()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.img_data_all.size(dim=0)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.img_data_all[idx], self.gt_bboxes_all[idx], self.gt_classes_all[idx]\n",
    "        \n",
    "    def get_data(self):\n",
    "        img_data_all = []\n",
    "        gt_idxs_all = []\n",
    "        \n",
    "        if img_dir is None:\n",
    "            gt_boxes_all, gt_classes_all, img_paths = parse_annotation(self.annotation_path, self.img_size)\n",
    "        else:\n",
    "            gt_boxes_all, gt_classes_all, img_paths = parse_annotation(self.annotation_path, self.img_size, img_dir = img_dir)\n",
    "        \n",
    "        for i, img_path in tqdm(enumerate(img_paths), total=len(img_paths)):\n",
    "      \n",
    "            # skip if the image path is not valid\n",
    "            if (not img_path) or (not os.path.exists(img_path)):\n",
    "                continue\n",
    "            \n",
    "            # read and resize image\n",
    "            \n",
    "            img = io.imread(img_path)\n",
    "            img = resize(img, self.img_size)\n",
    "            \n",
    "            # convert image to torch tensor and reshape it so channels come first\n",
    "            img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
    "            \n",
    "            # encode class names as integers\n",
    "            gt_classes = gt_classes_all[i]\n",
    "            gt_idx = torch.Tensor([self.name2idx[name] for name in gt_classes])\n",
    "            \n",
    "            img_data_all.append(img_tensor)\n",
    "            gt_idxs_all.append(gt_idx)\n",
    "        \n",
    "        # pad bounding boxes and classes so they are of the same size\n",
    "\n",
    "        if len(gt_boxes_all)!=0 and len(gt_idxs_all)!=0 :\n",
    "          \n",
    "          gt_bboxes_pad = pad_sequence(gt_boxes_all, batch_first=True, padding_value=-1)\n",
    "          gt_classes_pad = pad_sequence(gt_idxs_all, batch_first=True, padding_value=-1)\n",
    "        \n",
    "        # stack all images\n",
    "        print(img_data_all)\n",
    "        img_data_stacked = torch.stack(img_data_all)[:, :3, :, :]\n",
    "        \n",
    "        return img_data_stacked.to(dtype=torch.float32), gt_bboxes_pad, gt_classes_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = 640\n",
    "img_height = 480\n",
    "csv_path = \"/content/APS360_Traffic_Sign_Recognition/dataset_traffic_signs.csv\"\n",
    "image_dir = os.path.join(\"data\", \"images\")\n",
    "name2idx = {'pad': -1, '30kmh': 0,'60kmh':1, '100kmh' : 2, 'yield': 3, 'keepRight' :4, 'NoEntry':5, 'NoLeft': 6, 'Stop':7, 'noRight':8, 'ChildrenCrossing' :9 }\n",
    "idx2name = {v:k for k, v in name2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = \"C:/Users/gabri/Documents/UofT/APS360/Project/Code2/APS360_Traffic_Sign_Recognition/train.csv\"\n",
    "val_csv_path = \"C:/Users/gabri/Documents/UofT/APS360/Project/Code2/APS360_Traffic_Sign_Recognition/val.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"C:/Users/gabri/Documents/UofT/APS360/Project/Code2/APS360_Traffic_Sign_Recognition/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 1282/2243 [06:29<04:52,  3.29it/s]  \n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 9.38 MiB for an array with shape (480, 640, 4) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26276\\893530524.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mod_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mObjectDetectionDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_csv_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimg_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname2idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26276\\714779941.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, csv_path, img_size, name2idx, img_dir)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname2idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname2idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_data_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgt_bboxes_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgt_classes_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26276\\714779941.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;31m# convert image to torch tensor and reshape it so channels come first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[1;31m# The grid_mode kwarg was introduced in SciPy 1.6.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mzoom_factors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfactors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         out = ndi.zoom(image, zoom_factors, order=order, mode=ndi_mode,\n\u001b[0m\u001b[0;32m    188\u001b[0m                        cval=cval, grid_mode=True)\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\ndimage\\_interpolation.py\u001b[0m in \u001b[0;36mzoom\u001b[1;34m(input, zoom, output, order, mode, cval, prefilter, grid_mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m             [int(round(ii * jj)) for ii, jj in zip(input.shape, zoom)])\n\u001b[0;32m    770\u001b[0m     \u001b[0mcomplex_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miscomplexobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m     output = _ni_support._get_output(output, input, shape=output_shape,\n\u001b[0m\u001b[0;32m    772\u001b[0m                                      complex_output=complex_output)\n\u001b[0;32m    773\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcomplex_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\ndimage\\_ni_support.py\u001b[0m in \u001b[0;36m_get_output\u001b[1;34m(output, input, shape, complex_output)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcomplex_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mcomplex_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpromote_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomplex64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 9.38 MiB for an array with shape (480, 640, 4) and data type float64"
     ]
    }
   ],
   "source": [
    "od_dataset = ObjectDetectionDataset(train_csv_path, (img_height, img_width), name2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.read_csv(val_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "922"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
