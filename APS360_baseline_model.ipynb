{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jKO1qYcwW2tq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtVdlLMiUnJ4",
        "outputId": "87ce0ee2-212e-4fe4-8052-16cb1dde7cc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#mount googledrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drxPJW-NUwHa",
        "outputId": "03c017dc-bd83-48e3-cd18-8c676b35780b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/gdrive/MyDrive/UofT/Project/SecondData/Augdata_split.zip\n",
            "replace my_data/Augdata_split/test/30kmh/Classimg1036.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "#dezipping\n",
        "!unzip /content/gdrive/MyDrive/UofT/Project/SecondData/Augdata_split.zip -d my_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v4dSx7LuWxsx"
      },
      "outputs": [],
      "source": [
        "# location on Google Drive\n",
        "master_path = '/content/my_data/Augdata_split/'\n",
        "\n",
        "# Transform Settings - Do not use RandomResizedCrop\n",
        "transform = transforms.Compose([transforms.Resize((224,224)), \n",
        "                                transforms.ToTensor()])\n",
        "\n",
        "# assumes three folders with 60% training, 20% validation and 20% testing samples\n",
        "train_dataset = torchvision.datasets.ImageFolder(master_path + 'train', transform=transform)\n",
        "val_dataset = torchvision.datasets.ImageFolder(master_path + 'val', transform=transform)\n",
        "test_dataset = torchvision.datasets.ImageFolder(master_path + 'test', transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-fZ5HFWrWrm5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "\n",
        "def get_images_and_labels(dataset):\n",
        "    \"\"\"\n",
        "    Returns a list of all the images in a torchvision.datasets object\n",
        "    and a list of all the corresponding labels.\n",
        "\n",
        "    Args:\n",
        "        dataset: A torchvision.datasets object.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing two lists: the first list is the list of all the images in the dataset,\n",
        "        and the second list is the list of all the corresponding labels.\n",
        "    \"\"\"\n",
        "    # Initialize empty lists to store images and labels\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    # Loop over all data points in the dataset\n",
        "    for i in range(len(dataset)):\n",
        "        # Get the image and label for the current data point\n",
        "        image, label = dataset[i]\n",
        "        image = image.permute(1, 2, 0)\n",
        "        img = image.numpy() \n",
        "        # Append the image and label to the corresponding lists\n",
        "        images.append(img\n",
        "                      )\n",
        "        labels.append(label)\n",
        "\n",
        "    return images, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lwJ9hiBfXBP8"
      },
      "outputs": [],
      "source": [
        "train_images, train_labels = get_images_and_labels(train_dataset)\n",
        "val_images, val_labels = get_images_and_labels(val_dataset)\n",
        "test_images, test_labels = get_images_and_labels(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8vd6pmyYLHf",
        "outputId": "c559179f-76f3-4b3b-fde6-b63cb6546473"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(val_images[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jwmZ4pYdbtpb"
      },
      "outputs": [],
      "source": [
        "import numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7xb-7q95U30W"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.feature import hog\n",
        "\n",
        "# Load the dataset of images and labels\n",
        "# Here, we assume that the images are stored in a list called \"images\" and the corresponding labels are stored in a list called \"labels\"\n",
        "# Each image is a numpy array of shape (height, width, channels)\n",
        "\n",
        "\n",
        "# Extract HOG features from each image\n",
        "hog_features_train = []\n",
        "for image in train_images:\n",
        "    # Convert the image to grayscale\n",
        "    gray_image = cv2.cvtColor(numpy.asarray(image), cv2.COLOR_BGR2GRAY)\n",
        "    # Compute the HOG features\n",
        "    features = hog(gray_image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), block_norm='L2-Hys', transform_sqrt=True)\n",
        "    hog_features_train.append(features)\n",
        "\n",
        "hog_features_val = []\n",
        "for image in val_images:\n",
        "    # Convert the image to grayscale\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    # Compute the HOG features\n",
        "    features = hog(gray_image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), block_norm='L2-Hys', transform_sqrt=True)\n",
        "    hog_features_val.append(features)\n",
        "\n",
        "\n",
        "# Create an SVM classifier with a linear kernel\n",
        "clf = svm.SVC(kernel='linear', C=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "bZZ8o5FEex5U",
        "outputId": "040109d7-e3d7-495d-cbc6-22fc3a464552"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=1, kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1, kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "SVC(C=1, kernel='linear')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the classifier on the training set\n",
        "clf.fit(hog_features_train, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-7Ddd0Zg0yh"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy:\", clf.score(hog_features_train, train_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQeTSX8DYD0y"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Predict the labels of the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Print the accuracy score of the classifier\n",
        "print(\"Accuracy:\", clf.score(hog_features_train, train_labels)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
