{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetectionDataset(Dataset):\n",
    "    '''\n",
    "    A Pytorch Dataset class to load the images and their corresponding annotations.\n",
    "    \n",
    "    Returns\n",
    "    ------------\n",
    "    images: torch.Tensor of size (B, C, H, W)\n",
    "    gt bboxes: torch.Tensor of size (B, max_objects, 4)\n",
    "    gt classes: torch.Tensor of size (B, max_objects)\n",
    "    '''\n",
    "    def __init__(self, csv_path, img_size, name2idx):\n",
    "        self.annotation_path = csv_path\n",
    "        self.img_size = img_size\n",
    "        self.name2idx = name2idx\n",
    "        \n",
    "        self.img_data_all, self.gt_bboxes_all, self.gt_classes_all = self.get_data()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.img_data_all.size(dim=0)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.img_data_all[idx], self.gt_bboxes_all[idx], self.gt_classes_all[idx]\n",
    "        \n",
    "    def get_data(self):\n",
    "        img_data_all = []\n",
    "        gt_idxs_all = []\n",
    "        \n",
    "        gt_boxes_all, gt_classes_all, img_paths = parse_annotation(self.annotation_path, self.img_size)\n",
    "        \n",
    "        for i, img_path in tqdm(enumerate(img_paths), total=len(img_paths)):\n",
    "      \n",
    "            # skip if the image path is not valid\n",
    "            if (not img_path) or (not os.path.exists(img_path)):\n",
    "                continue\n",
    "            \n",
    "            # read and resize image\n",
    "            \n",
    "            img = io.imread(img_path)\n",
    "            img = resize(img, self.img_size)\n",
    "            \n",
    "            # convert image to torch tensor and reshape it so channels come first\n",
    "            img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
    "            \n",
    "            # encode class names as integers\n",
    "            gt_classes = gt_classes_all[i]\n",
    "            gt_idx = torch.Tensor([self.name2idx[name] for name in gt_classes])\n",
    "            \n",
    "            img_data_all.append(img_tensor)\n",
    "            gt_idxs_all.append(gt_idx)\n",
    "        \n",
    "        # pad bounding boxes and classes so they are of the same size\n",
    "\n",
    "        if len(gt_boxes_all)!=0 and len(gt_idxs_all)!=0 :\n",
    "          \n",
    "          gt_bboxes_pad = pad_sequence(gt_boxes_all, batch_first=True, padding_value=-1)\n",
    "          gt_classes_pad = pad_sequence(gt_idxs_all, batch_first=True, padding_value=-1)\n",
    "        \n",
    "        # stack all images\n",
    "        print(img_data_all)\n",
    "        img_data_stacked = torch.stack(img_data_all)[:, :3, :, :]\n",
    "        \n",
    "        return img_data_stacked.to(dtype=torch.float32), gt_bboxes_pad, gt_classes_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = 640\n",
    "img_height = 480\n",
    "csv_path = \"/content/APS360_Traffic_Sign_Recognition/dataset_traffic_signs.csv\"\n",
    "image_dir = os.path.join(\"data\", \"images\")\n",
    "name2idx = {'pad': -1, '30kmh': 0,'60kmh':1, '100kmh' : 2, 'yield': 3, 'keepRight' :4, 'NoEntry':5, 'NoLeft': 6, 'Stop':7, 'noRight':8, 'ChildrenCrossing' :9 }\n",
    "idx2name = {v:k for k, v in name2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = \"C:/Users/gabri/Documents/UofT/APS360/Project/Code2/APS360_Traffic_Sign_Recognition/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2243/2243 [00:00<00:00, 51946.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14036\\893530524.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mod_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mObjectDetectionDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_csv_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimg_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname2idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14036\\1178027876.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, csv_path, img_size, name2idx)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname2idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname2idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_data_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgt_bboxes_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgt_classes_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14036\\1178027876.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;31m# stack all images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_data_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mimg_data_stacked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_data_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg_data_stacked\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_bboxes_pad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_classes_pad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "od_dataset = ObjectDetectionDataset(train_csv_path, (img_height, img_width), name2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
